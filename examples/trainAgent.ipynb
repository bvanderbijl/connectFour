{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on how to train a DQN for the created connfect four encironment\n",
    "This notebook, shows and explains how an AI model can be used to learn how to play the game connect four using reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/basvanderbijl/Library/Mobile Documents/com~apple~CloudDocs/DataGreen/projects/connectFour/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join('..', 'code'))\n",
    "from connectFour import *\n",
    "from connectFourGame import *\n",
    "from agents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ConnectFourGymEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.flatten().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = env.observation_space.flatten().shape\n",
    "num_actions = len(env.action_space)\n",
    "\n",
    "agent = DQNAgent(input_shape, num_actions, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 20)                860       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                1050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 357       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2267 (8.86 KB)\n",
      "Trainable params: 2267 (8.86 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "batch_size = 100\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, (1,-1))\n",
    "    \n",
    "    while not env.game_over:\n",
    "        action = agent.make_move(env.agent.current_player, state, env.action_space)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if env.agent.current_player == 0:\n",
    "            agent.memorize(state, action, reward, new_state, done)\n",
    "        state = new_state\n",
    "        state = np.reshape(state, (1,-1))\n",
    "    \n",
    "    rewards.append(reward)\n",
    "        \n",
    "    agent.learn(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"agentv1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
